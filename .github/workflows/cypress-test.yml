name: Pre merge cypress tests

on:
  pull_request:
    paths:
      - 'cypress/e2e/**'
      - 'cypress/support/**'

jobs:
  run-tests:
    name: Run Cypress Tests
    permissions:
      contents: read
      pull-requests: write
    runs-on: ubuntu-latest
    steps:
      - name: Clone repository
        uses: actions/checkout@v3
        with:
          fetch-depth: 1

      - name: Use Node.js 18.x
        uses: actions/setup-node@v3
        with:
          node-version: 18.x

      - name: Install dependencies
        run: npm install

      - name: Run Cypress tests
        run: npm run test-ui-headless

      - name: Upload Cypress test report
        uses: actions/upload-artifact@v3
        with:
          name: cypress-report
          path: |
            cypress/reports/merged-report.html
            cypress/reports/assets/

      - name: Discover report files
        id: discover-reports
        run: |
          REPORT_FILES=$(find cypress/reports -type f -name 'merged-report.json')
          echo "REPORT_FILES=${REPORT_FILES}" >> $GITHUB_ENV

      - name: Extract and group test data
        id: extract-test-data
        run: |
          # Initialize Markdown file
          echo "| **Test Type** | **Total Tests** | **Passed** | **Failed** |" > test-report-summary.md
          echo "|--------------|-----------------|------------|------------|" >> test-report-summary.md

          # Initialize counters
          api_total=0
          api_passed=0
          api_failed=0
          ui_total=0
          ui_passed=0
          ui_failed=0

          echo "build_failed=false" >> $GITHUB_ENV
          echo "has_content=false" >> $GITHUB_ENV

          for report_file in ${REPORT_FILES}; do
            if [ -f "$report_file" ]; then
              echo "has_content=true" >> $GITHUB_ENV
              
              # Process each feature file in results
              jq -c '.results[]' "$report_file" | while read -r feature; do
                file=$(echo "$feature" | jq -r '.file')
                
                # Initialize per-feature counters
                feature_total=0
                feature_passed=0
                feature_failed=0

                # Loop through suites
                echo "$feature" | jq -c '.suites[]' | while read -r suite; do
                  suite_total=$(echo "$suite" | jq '.tests | length')
                  suite_passed=$(echo "$suite" | jq '[.tests[] | select(.pass == true)] | length')
                  suite_failed=$(echo "$suite" | jq '[.tests[] | select(.fail == true)] | length')

                  # Add suite results to feature-level counters
                  feature_total=$((feature_total + suite_total))
                  feature_passed=$((feature_passed + suite_passed))
                  feature_failed=$((feature_failed + suite_failed))
                done

                # Update global counters based on test type
                if [[ "$file" == *"apiTests"* ]]; then
                  api_total=$((api_total + feature_total))
                  api_passed=$((api_passed + feature_passed))
                  api_failed=$((api_failed + feature_failed))
                elif [[ "$file" == *"uiTests"* ]]; then
                  ui_total=$((ui_total + feature_total))
                  ui_passed=$((ui_passed + feature_passed))
                  ui_failed=$((ui_failed + feature_failed))
                fi
              done
              
              # Check for failed tests
              if [ "$api_failed" -ne 0 ] || [ "$ui_failed" -ne 0 ]; then
                echo "build_failed=true" >> $GITHUB_ENV
              fi
            fi
          done

          # Append results to Markdown file
          echo "| API Tests    | $api_total      | $api_passed  | $api_failed  |" >> test-report-summary.md
          echo "| UI Tests     | $ui_total       | $ui_passed   | $ui_failed   |" >> test-report-summary.md

      - name: Comment on PR with test report summary
        if: env.has_content == 'true'
        uses: marocchino/sticky-pull-request-comment@v2
        with:
          path: test-report-summary.md
          recreate: true

      - name: Fail build if tests failed
        if: env.build_failed == 'true'
        run: |
          echo "Some tests failed. Failing the build."
          exit 1
